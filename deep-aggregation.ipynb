{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7941614,"sourceType":"datasetVersion","datasetId":4669222},{"sourceId":20165,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":16734}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.io import read_image\nfrom itertools import chain\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nfrom PIL import Image\n\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_fscore_support\n\nfrom torch import nn, optim\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport random\nfrom copy import deepcopy\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:04:29.409856Z","iopub.execute_input":"2024-03-28T01:04:29.410107Z","iopub.status.idle":"2024-03-28T01:04:44.792615Z","shell.execute_reply.started":"2024-03-28T01:04:29.410084Z","shell.execute_reply":"2024-03-28T01:04:44.791861Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# from tempfile import TemporaryDirectory\ncudnn.benchmark = True\n# import pdb","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:04:59.062729Z","iopub.execute_input":"2024-03-28T01:04:59.063228Z","iopub.status.idle":"2024-03-28T01:04:59.067597Z","shell.execute_reply.started":"2024-03-28T01:04:59.063201Z","shell.execute_reply":"2024-03-28T01:04:59.066643Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:04:59.999756Z","iopub.execute_input":"2024-03-28T01:05:00.000074Z","iopub.status.idle":"2024-03-28T01:05:00.064066Z","shell.execute_reply.started":"2024-03-28T01:05:00.000048Z","shell.execute_reply":"2024-03-28T01:05:00.062993Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"class AllDataset(Dataset):\n    def __init__(self, img_dir, annotations, transform=None):\n        \"\"\"\n        Naive implementation:\n        List all the images in the given directory (trainset/testset)\n        \n        For all subfolder in root:\n            For each image in the subfolder:\n                Add image path to the list\n        \"\"\"\n        self.img_dir = img_dir\n        self.annotations = pd.read_csv(annotations)\n        self.transform = transform\n        \n        self.img_paths = [glob.glob(os.path.join(img_dir + '/' + subfolder, '*.jpg')) for subfolder in os.listdir(img_dir)]\n        self.img_paths = list(chain.from_iterable(self.img_paths))\n        \n        \n        \n    def __len__(self):\n        return len(self.img_paths)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Read the image, extract the pid from the image path\n        Locate the corresponding label, gender, lymph_count\n        And return all of them\n        \n        To do: what to do with DOB\n        options: year, or the unix age\n        \"\"\"\n        image = read_image(self.img_paths[idx])\n        PID = self.img_paths[idx].split('/')[-2]\n        label = list(self.annotations[self.annotations['ID'] == PID]['LABEL'])[0]\n        gender = list(self.annotations[self.annotations['ID'] == PID]['GENDER'])[0]\n#         print(type(gender)) -- bug: it should be string, but the dataloader returns a tuple ('M', )\n        lymph_count = list(self.annotations[self.annotations['ID'] == PID]['LYMPH_COUNT'])[0]\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label, gender, lymph_count, PID","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:00.747669Z","iopub.execute_input":"2024-03-28T01:05:00.747956Z","iopub.status.idle":"2024-03-28T01:05:00.757712Z","shell.execute_reply.started":"2024-03-28T01:05:00.747933Z","shell.execute_reply":"2024-03-28T01:05:00.756890Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class BagDataset(Dataset):\n    \"\"\"This dataset include the images and metadata of a list of subjects\"\"\"\n\n    def __init__(self, img_dir, data_df, transform=None):\n        \"\"\"\n        Args:\n            img_dir: (str) path to the images directory.\n            data_df: (DataFrame) list of subjects used.\n            transform: Optional, transformations applied to the tensor\n        \"\"\"\n        self.img_dir = img_dir\n        self.transform = transform\n        self.data_df = data_df\n\n    def __len__(self):\n        return len(self.data_df)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx: (int) the index of the subject/session whom data is loaded.\n        Returns:\n            sample: (dict) corresponding data described by the following keys:\n                images: (List) List with image tensors\n                label: (int) the diagnosis code\n                id: (str) ID of the participant\n                gender: (str) 'M' or 'F'\n                age: (int) age\n                lymph_count: (int) lymph count\n        \"\"\"\n\n        label = self.data_df.loc[idx, 'LABEL']\n        age = self.data_df.loc[idx, 'AGE']\n        gender = self.data_df.loc[idx, 'GENDER']\n        lymph_count = self.data_df.loc[idx, 'LYMPH_COUNT']\n\n        id = self.data_df.loc[idx, 'ID']\n        folder_name = os.path.join(self.img_dir, id)\n        images = []\n        for filename in Path(folder_name).glob('*'):\n            image = read_image(str(filename))\n            \n            if self.transform:\n                image = self.transform(image)\n            \n            images.append(image)\n\n        sample = {'images': images, 'label': label,\n                  'id': id, 'gender': gender, 'age': age,\n                  'lymph_count': lymph_count}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:02.239487Z","iopub.execute_input":"2024-03-28T01:05:02.239832Z","iopub.status.idle":"2024-03-28T01:05:02.249376Z","shell.execute_reply.started":"2024-03-28T01:05:02.239805Z","shell.execute_reply":"2024-03-28T01:05:02.248525Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x\n\nclass FCLayer(nn.Module):\n    def __init__(self, in_size, out_size=1):\n        super(FCLayer, self).__init__()\n        self.fc = nn.Sequential(nn.Linear(in_size, out_size))\n    def forward(self, feats):\n        x = self.fc(feats)\n        return feats, x\n\nclass IClassifier(nn.Module):\n    def __init__(self, feature_extractor, feature_size, output_class):\n        super(IClassifier, self).__init__()\n        \n        self.feature_extractor = feature_extractor      \n        self.fc = nn.Linear(feature_size, output_class)\n        \n        \n    def forward(self, x):\n        device = x.device\n        feats = self.feature_extractor(x) # N x K\n        c = self.fc(feats.view(feats.shape[0], -1)) # N x C\n        return feats.view(feats.shape[0], -1), c\n\nclass BClassifier(nn.Module):\n    def __init__(self, input_size, output_class, dropout_v=0.0, nonlinear=True, passing_v=False): # K, L, N\n        super(BClassifier, self).__init__()\n        if nonlinear:\n            self.q = nn.Sequential(nn.Linear(input_size, 128), nn.ReLU(), nn.Linear(128, 128), nn.Tanh())\n        else:\n            self.q = nn.Linear(input_size, 128)\n        if passing_v:\n            self.v = nn.Sequential(\n                nn.Dropout(dropout_v),\n                nn.Linear(input_size, input_size),\n                nn.ReLU()\n            )\n        else:\n            self.v = nn.Identity()\n        \n        ### 1D convolutional layer that can handle multiple class (including binary)\n        self.fcc = nn.Conv1d(output_class, output_class, kernel_size=input_size)  \n        \n    def forward(self, feats, c): # N x K, N x C\n        device = feats.device\n        V = self.v(feats) # N x V, unsorted\n        Q = self.q(feats).view(feats.shape[0], -1) # N x Q, unsorted\n        \n        # handle multiple classes without for loop\n        _, m_indices = torch.sort(c, 0, descending=True) # sort class scores along the instance dimension, m_indices in shape N x C\n        m_feats = torch.index_select(feats, dim=0, index=m_indices[0, :]) # select critical instances, m_feats in shape C x K \n        q_max = self.q(m_feats) # compute queries of critical instances, q_max in shape C x Q\n        A = torch.mm(Q, q_max.transpose(0, 1)) # compute inner product of Q to each entry of q_max, A in shape N x C, each column contains unnormalized attention scores\n        A = F.softmax( A / torch.sqrt(torch.tensor(Q.shape[1], dtype=torch.float32, device=device)), 0) # normalize attention scores, A in shape N x C, \n        B = torch.mm(A.transpose(0, 1), V) # compute bag representation, B in shape C x V\n                \n        B = B.view(1, B.shape[0], B.shape[1]) # 1 x C x V\n        C = self.fcc(B) # 1 x C x 1\n        C = C.view(1, -1)\n        return C, A, B \n    \n\nclass FullNet(nn.Module):\n    def __init__(self, i_classifier, b_classifier, feature_extractor):\n        super(FullNet, self).__init__()\n        self.feature_extractor = feature_extractor\n#         self.resnet18 = models.resnet18(weights=None)\n#         if PATH:\n#             self.resnet18.load_state_dict(torch.load(PATH))\n#         self.resnet18.fc = Identity()\n        self.i_classifier = i_classifier\n        self.b_classifier = b_classifier\n#         self.fc1 = nn.Linear(2, 128)\n#         self.relu = nn.ReLU()\n#         self.fc2 = nn.Linear(128, 128)\n#         self.fc3 = nn.Linear(128, 64)\n#         self.fc4 = nn.Linear(64, 1)\n            \n    def forward(self, x):\n        feats = self.feature_extractor(x)\n#         max_value, min_value = torch.max(feats), torch.min(feats)\n#         feats = (feats - min_value) / (max_value - min_value)\n        \n        feats, classes = self.i_classifier(feats)\n        prediction_bag, A, B = self.b_classifier(feats, classes)\n#         max_prediction, index = torch.max(classes, 0)\n#         output = self.relu(self.fc1(torch.tensor([[prediction_bag, max_prediction]]).to(device)))\n#         output = self.relu(self.fc2(output))\n#         output = self.relu(self.fc3(output))\n#         output = self.relu(self.fc4(output))\n        \n        return classes, prediction_bag, A, B","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:02.635951Z","iopub.execute_input":"2024-03-28T01:05:02.636200Z","iopub.status.idle":"2024-03-28T01:05:02.655375Z","shell.execute_reply.started":"2024-03-28T01:05:02.636179Z","shell.execute_reply":"2024-03-28T01:05:02.654427Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# -- Encoder - Decoder\nclass CNN_Encoder(nn.Module):\n    def __init__(self, output_size, input_size=(3, 64, 64)):\n        super(CNN_Encoder, self).__init__()\n\n        self.input_size = input_size\n        self.channel_mult = 16\n\n        #convolutions\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, self.channel_mult*1, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(self.channel_mult*1, self.channel_mult*2, 4, 2, 1),\n            nn.BatchNorm2d(self.channel_mult*2),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(self.channel_mult*2, self.channel_mult*4, 4, 2, 1),\n            nn.BatchNorm2d(self.channel_mult*4),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(self.channel_mult*4, self.channel_mult*8, 4, 2, 1),\n            nn.BatchNorm2d(self.channel_mult*8),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(self.channel_mult*8, self.channel_mult*16, 3, 2, 1),\n            nn.BatchNorm2d(self.channel_mult*16),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(self.channel_mult*16, self.channel_mult*16, 3, 1, 1),\n            nn.BatchNorm2d(self.channel_mult*16),\n            nn.LeakyReLU(0.2, inplace=True)\n\n        )\n\n        self.flat_fts = self.get_flat_fts(self.conv)\n\n        self.linear = nn.Sequential(\n            nn.Linear(self.flat_fts, int(self.flat_fts/2)),\n            nn.Linear(int(self.flat_fts/2), output_size),\n            nn.BatchNorm1d(output_size),\n            nn.LeakyReLU(0.2),\n        )\n\n    def get_flat_fts(self, fts):\n        f = fts(Variable(torch.ones(1, 3, 64, 64)))\n        return int(np.prod(f.size()[1:]))\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(-1, self.flat_fts)\n        return self.linear(x)\n\nclass CNN_Encoder_Resnet(nn.Module):\n    def __init__(self, output_size, input_size=(3, 64, 64)):\n        super(CNN_Encoder_Resnet, self).__init__()\n        self.resnet = models.resnet18()\n        self.flat_fts = 512\n        self.resnet.fc = nn.Sequential(\n            nn.Linear(self.flat_fts, int(self.flat_fts/2)),\n            nn.Linear(int(self.flat_fts/2), output_size),\n            nn.BatchNorm1d(output_size),\n            nn.LeakyReLU(0.2),\n        )\n\n    def get_flat_fts(self, fts):\n        f = fts(Variable(torch.ones(1, 3, 64, 64)))\n        return int(np.prod(f.size()[1:]))\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return x\n\nclass CNN_Decoder(nn.Module):\n    def __init__(self, embedding_size=256, input_size=(3, 64, 64)):\n        super(CNN_Decoder, self).__init__()\n        self.input_height = input_size[1]\n        self.input_width = input_size[2]\n        self.input_dim = embedding_size\n        self.channel_mult = 16\n        self.output_channels = input_size[0]\n        self.fc_output_dim = 4096\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.input_dim, self.fc_output_dim),\n            nn.BatchNorm1d(self.fc_output_dim),\n            nn.ReLU(True)\n        )\n\n        self.deconv = nn.Sequential(\n            nn.ConvTranspose2d(self.fc_output_dim, self.channel_mult*8,\n                                4, 1, 0, bias=False),  # Adjusted kernel, stride, padding\n            nn.BatchNorm2d(self.channel_mult*8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(self.channel_mult*8, self.channel_mult*4,\n                                4, 2, 1, bias=False),  # Adjusted kernel, stride, padding\n            nn.BatchNorm2d(self.channel_mult*4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(self.channel_mult*4, self.channel_mult*2,\n                                4, 2, 1, bias=False),  # Adjusted kernel, stride, padding\n            nn.BatchNorm2d(self.channel_mult*2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(self.channel_mult*2, self.channel_mult,\n                                4, 2, 1, bias=False),  # Adjusted kernel, stride, padding\n            nn.BatchNorm2d(self.channel_mult),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(self.channel_mult, self.output_channels,\n                                4, 2, 1, bias=False),  # Adjusted kernel, stride, padding\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(-1, self.fc_output_dim, 1, 1)\n        x = self.deconv(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:03.069467Z","iopub.execute_input":"2024-03-28T01:05:03.069994Z","iopub.status.idle":"2024-03-28T01:05:03.093487Z","shell.execute_reply.started":"2024-03-28T01:05:03.069971Z","shell.execute_reply":"2024-03-28T01:05:03.092659Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self, embedding_size=256):\n        super(Network, self).__init__()\n        self.encoder = CNN_Encoder_Resnet(embedding_size) # , input_size=(3, 224, 224))\n        self.decoder = CNN_Decoder(embedding_size) #, input_size=(3, 224, 224))\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def decode(self, z):\n        return self.decoder(z)\n\n    def forward(self, x):\n        z = self.encode(x)\n        return z, self.decode(z)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:03.762027Z","iopub.execute_input":"2024-03-28T01:05:03.762302Z","iopub.status.idle":"2024-03-28T01:05:03.768625Z","shell.execute_reply.started":"2024-03-28T01:05:03.762279Z","shell.execute_reply":"2024-03-28T01:05:03.767586Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_my_state_dict(model, state_dict):\n\n    own_state = model.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n             continue\n        if isinstance(param, nn.Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n        own_state[name].copy_(param)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:04.281439Z","iopub.execute_input":"2024-03-28T01:05:04.282093Z","iopub.status.idle":"2024-03-28T01:05:04.286735Z","shell.execute_reply.started":"2024-03-28T01:05:04.282069Z","shell.execute_reply":"2024-03-28T01:05:04.285880Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(64),\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        # transforms.RandomRotation(5)\n        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:05.428103Z","iopub.execute_input":"2024-03-28T01:05:05.428555Z","iopub.status.idle":"2024-03-28T01:05:05.433479Z","shell.execute_reply.started":"2024-03-28T01:05:05.428528Z","shell.execute_reply":"2024-03-28T01:05:05.432573Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"enc_dataset = AllDataset(img_dir='/kaggle/input/classification-dataset/dlmi-lymphocytosis-classification/trainset',\n                        annotations='/kaggle/input/classification-dataset/dlmi-lymphocytosis-classification/clinical_annotation.csv',\n                        transform=transform)\n\nVAL_SPLIT_RATIO = 0.2\n\nenc_dataset_size = len(enc_dataset)\nenc_dataset_indices = list(range(enc_dataset_size))\n\nval_split_index = int(np.floor(VAL_SPLIT_RATIO * enc_dataset_size))\n\ntrain_idx, val_idx = enc_dataset_indices[val_split_index:], enc_dataset_indices[:val_split_index]\n\nenc_train_sampler = SubsetRandomSampler(train_idx)\nenc_val_sampler = SubsetRandomSampler(val_idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:08.053307Z","iopub.execute_input":"2024-03-28T01:05:08.054158Z","iopub.status.idle":"2024-03-28T01:05:15.316187Z","shell.execute_reply.started":"2024-03-28T01:05:08.054116Z","shell.execute_reply":"2024-03-28T01:05:15.315339Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"enc_trainloader = DataLoader(dataset=enc_dataset, batch_size=128, sampler=enc_train_sampler, num_workers=4)\n\nenc_valloader = DataLoader(dataset=enc_dataset, batch_size=128, sampler=enc_val_sampler, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:15.317757Z","iopub.execute_input":"2024-03-28T01:05:15.318066Z","iopub.status.idle":"2024-03-28T01:05:15.323106Z","shell.execute_reply.started":"2024-03-28T01:05:15.318043Z","shell.execute_reply":"2024-03-28T01:05:15.322130Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# enc_model = Network()\n\n# enc_model = enc_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:15.324464Z","iopub.execute_input":"2024-03-28T01:05:15.324825Z","iopub.status.idle":"2024-03-28T01:05:15.334919Z","shell.execute_reply.started":"2024-03-28T01:05:15.324795Z","shell.execute_reply":"2024-03-28T01:05:15.334037Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# enc_criterion = nn.MSELoss()\n# enc_optimizer = torch.optim.Adam(enc_model.parameters(), lr=1e-2) # , betas=(0.9, 0.999), weight_decay=1e-4)\n# # optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, 0)\n# enc_scheduler = torch.optim.lr_scheduler.MultiStepLR(enc_optimizer, milestones=[10,20,40], gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:15.336983Z","iopub.execute_input":"2024-03-28T01:05:15.337533Z","iopub.status.idle":"2024-03-28T01:05:15.346771Z","shell.execute_reply.started":"2024-03-28T01:05:15.337503Z","shell.execute_reply":"2024-03-28T01:05:15.346047Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def test_enc(model, dataloader, criterion):\n#     model.eval()\n#     with torch.no_grad():\n#         total_loss = 0\n#         for images, _, _, _, _ in dataloader:\n#             images = images.to(device)\n#             _, recons = model(images)\n#             loss = 100 * criterion(images, recons)\n            \n#             total_loss += loss.item()\n        \n#         print(\"Test Loss: {}\".format(total_loss / len(dataloader)))\n            ","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:15.347688Z","iopub.execute_input":"2024-03-28T01:05:15.347928Z","iopub.status.idle":"2024-03-28T01:05:15.359283Z","shell.execute_reply.started":"2024-03-28T01:05:15.347908Z","shell.execute_reply":"2024-03-28T01:05:15.358416Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# def train_enc(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler):\n#     for epoch in range(epochs):\n#         model.train()\n#         epoch_loss = 0\n#         for images, _, _, _, _ in train_loader:\n#             images = images.to(device)\n#             _, recons = model(images)\n# #             print(recons.shape)\n#             loss = 100 * criterion(images, recons)\n            \n#             epoch_loss += loss.item()\n#             optimizer.zero_grad()\n#             loss.backward()\n#             optimizer.step()\n        \n#         print(\"Train Loss: {}\".format(epoch_loss / len(train_loader)))\n        \n# #         test_enc(model, train_loader, criterion)\n#         test_enc(model, val_loader, criterion)\n        \n#         if epoch == 49:\n#             PATH = '/kaggle/working/enc50.pth'\n#             torch.save({\n#                 'epoch': epoch+1,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#             }, PATH)\n#         if epoch == 99:\n#             PATH = '/kaggle/working/enc100.pth'\n#             torch.save({\n#                 'epoch': epoch+1,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#             }, PATH)\n# #         scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:15.360319Z","iopub.execute_input":"2024-03-28T01:05:15.360635Z","iopub.status.idle":"2024-03-28T01:05:15.371724Z","shell.execute_reply.started":"2024-03-28T01:05:15.360606Z","shell.execute_reply":"2024-03-28T01:05:15.371012Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# train_enc(enc_model, enc_trainloader, enc_valloader, 100, enc_criterion, enc_optimizer, enc_scheduler)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:57.397612Z","iopub.execute_input":"2024-03-27T16:58:57.397951Z","iopub.status.idle":"2024-03-27T16:58:57.408742Z","shell.execute_reply.started":"2024-03-27T16:58:57.397921Z","shell.execute_reply":"2024-03-27T16:58:57.407593Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# images, _, _, _, _ = next(iter(enc_valloader))\n# test_image = images[1].unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:19.875529Z","iopub.execute_input":"2024-03-28T01:05:19.875881Z","iopub.status.idle":"2024-03-28T01:05:19.879739Z","shell.execute_reply.started":"2024-03-28T01:05:19.875854Z","shell.execute_reply":"2024-03-28T01:05:19.878761Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# with torch.no_grad():\n#     test_image = test_image.to(device)\n#     z, recon_image = enc_model(test_image)\n# #\n# print(z.shape, recon_image.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:21.630809Z","iopub.execute_input":"2024-03-28T01:05:21.631182Z","iopub.status.idle":"2024-03-28T01:05:21.635504Z","shell.execute_reply.started":"2024-03-28T01:05:21.631153Z","shell.execute_reply":"2024-03-28T01:05:21.634469Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# input_im = test_image.squeeze(0).detach().cpu().numpy()\n# plt.imshow(np.moveaxis(input_im, 0, 2))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:23.557187Z","iopub.execute_input":"2024-03-28T01:05:23.557854Z","iopub.status.idle":"2024-03-28T01:05:23.561445Z","shell.execute_reply.started":"2024-03-28T01:05:23.557826Z","shell.execute_reply":"2024-03-28T01:05:23.560576Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# out_np = recon_image.squeeze(0).detach().cpu().numpy()\n# plt.imshow(np.moveaxis(out_np,0,2))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:24.458774Z","iopub.execute_input":"2024-03-28T01:05:24.459344Z","iopub.status.idle":"2024-03-28T01:05:24.462817Z","shell.execute_reply.started":"2024-03-28T01:05:24.459319Z","shell.execute_reply":"2024-03-28T01:05:24.461920Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/simclr_pretrained_r18/pytorch/r18_online/1/simclr_pretrained.pth.tar","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:25.846893Z","iopub.execute_input":"2024-03-28T01:05:25.847241Z","iopub.status.idle":"2024-03-28T01:05:25.851513Z","shell.execute_reply.started":"2024-03-28T01:05:25.847214Z","shell.execute_reply":"2024-03-28T01:05:25.850568Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"feature_extractor = CNN_Encoder_Resnet(256)\n# feature_extractor.resnet.fc = nn.Sequential(nn.Linear(512, 256), nn.Linear(256, 256))\n\n# local_checkpoint = torch.load('/kaggle/working/enc100.pth')['model_state_dict']\n# feature_extractor.load_state_dict(local_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:27.878091Z","iopub.execute_input":"2024-03-28T01:05:27.878434Z","iopub.status.idle":"2024-03-28T01:05:28.202477Z","shell.execute_reply.started":"2024-03-28T01:05:27.878405Z","shell.execute_reply":"2024-03-28T01:05:28.201700Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# new_state_dict = OrderedDict()\n# count = 0\n# for k, v in local_checkpoint.items():\n#     count += 1\n#     name = k[8:] # remove `module.`\n# #     print(name)\n#     new_state_dict[name] = v\n    \n\n# load_my_state_dict(feature_extractor, new_state_dict)\n# # print(count)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:29.946378Z","iopub.execute_input":"2024-03-28T01:05:29.947128Z","iopub.status.idle":"2024-03-28T01:05:29.951094Z","shell.execute_reply.started":"2024-03-28T01:05:29.947098Z","shell.execute_reply":"2024-03-28T01:05:29.950056Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# with torch.no_grad():\n# #     feature_extractor.eval()\n#     test_image = test_image\n#     z = feature_extractor(test_image)\n# #\n# print(z[:10])","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:31.625151Z","iopub.execute_input":"2024-03-28T01:05:31.625463Z","iopub.status.idle":"2024-03-28T01:05:31.629361Z","shell.execute_reply.started":"2024-03-28T01:05:31.625437Z","shell.execute_reply":"2024-03-28T01:05:31.628350Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# PATH = \"/kaggle/input/simclr_pretrained_r18/pytorch/r18_online/1/simclr_pretrained.pth.tar\"\n# resnet = models.resnet50(weights=None)\n# resnet.fc = nn.Sequential(nn.Linear(2048, 512),nn.ReLU(), nn.Linear(512, 256))\n\n# # resnet18 = nn.DataParallel(resnet18)\n# # checkpoint = torch.load(PATH, map_location=torch.device('cpu'))['state_dict']\n# # checkpoint = torch.load(PATH)['state_dict']\n# # resnet18.load_state_dict(torch.load(PATH, map_location=torch.device('cpu'))['state_dict'])\n# # resnet18.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:31.999443Z","iopub.execute_input":"2024-03-28T01:05:31.999710Z","iopub.status.idle":"2024-03-28T01:05:32.003643Z","shell.execute_reply.started":"2024-03-28T01:05:31.999689Z","shell.execute_reply":"2024-03-28T01:05:32.002766Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# temp_net = models.resnet50()\n# print(temp_net)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:33.415437Z","iopub.execute_input":"2024-03-28T01:05:33.416156Z","iopub.status.idle":"2024-03-28T01:05:33.419499Z","shell.execute_reply.started":"2024-03-28T01:05:33.416128Z","shell.execute_reply":"2024-03-28T01:05:33.418691Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# new_state_dict = OrderedDict()\n# for k, v in checkpoint.items():\n#     name = k[14:] # remove `module.`\n#     new_state_dict[name] = v\n    \n\n# load_my_state_dict(resnet, new_state_dict)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:33.805943Z","iopub.execute_input":"2024-03-28T01:05:33.806219Z","iopub.status.idle":"2024-03-28T01:05:33.809890Z","shell.execute_reply.started":"2024-03-28T01:05:33.806198Z","shell.execute_reply":"2024-03-28T01:05:33.809011Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"img_path = '/kaggle/input/classification-dataset/dlmi-lymphocytosis-classification/trainset'\ntrain_df = pd.read_csv('/kaggle/input/classification-dataset/train_set.csv')\nval_df = pd.read_csv('/kaggle/input/classification-dataset/val_set.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:35.881147Z","iopub.execute_input":"2024-03-28T01:05:35.881481Z","iopub.status.idle":"2024-03-28T01:05:35.909066Z","shell.execute_reply.started":"2024-03-28T01:05:35.881457Z","shell.execute_reply":"2024-03-28T01:05:35.908354Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# class GaussianBlur(object):\n#     # Implements Gaussian blur as described in the SimCLR paper\n#     def __init__(self, kernel_size, min=0.1, max=2.0):\n#         self.min = min\n#         self.max = max\n#         # kernel size is set to be 10% of the image height/width\n#         self.kernel_size = kernel_size\n\n#     def __call__(self, sample):\n#         sample = np.array(sample)\n\n#         # blur the image with a 50% chance\n#         prob = np.random.random_sample()\n\n#         if prob < 0.5:\n# #            print(self.kernel_size)\n#             sigma = (self.max - self.min) * np.random.random_sample() + self.min\n#             sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n\n#         return sample","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:37.361566Z","iopub.execute_input":"2024-03-28T01:05:37.361904Z","iopub.status.idle":"2024-03-28T01:05:37.366293Z","shell.execute_reply.started":"2024-03-28T01:05:37.361881Z","shell.execute_reply":"2024-03-28T01:05:37.365427Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# train_transforms = transforms.Compose([transforms.ToPILImage(),\n#                                        transforms.Resize(128),\n# #                                        transforms.RandomResizedCrop(128),\n# #                                        transforms.RandomHorizontalFlip(),\n# #                                        transforms.RandomGrayscale(p=0.2),\n# #                                        GaussianBlur(kernel_size=int(0.06 * 224)),\n#                                        transforms.ToTensor(),\n#                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n#                                       ])\n\ntrain_transforms = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(64),\n        transforms.ToTensor(),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        # transforms.RandomRotation(5)\n        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\nval_transforms = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(64),\n        transforms.ToTensor(),\n#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:42.314681Z","iopub.execute_input":"2024-03-28T01:05:42.315017Z","iopub.status.idle":"2024-03-28T01:05:42.321374Z","shell.execute_reply.started":"2024-03-28T01:05:42.314992Z","shell.execute_reply":"2024-03-28T01:05:42.320446Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_dataset = BagDataset(img_path, train_df, train_transforms)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=1, num_workers=4, shuffle=True)\n\nval_dataset = BagDataset(img_path, val_df, val_transforms)\nval_loader = DataLoader(dataset=val_dataset, batch_size=1, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:05:46.452051Z","iopub.execute_input":"2024-03-28T01:05:46.452727Z","iopub.status.idle":"2024-03-28T01:05:46.457803Z","shell.execute_reply.started":"2024-03-28T01:05:46.452698Z","shell.execute_reply":"2024-03-28T01:05:46.456887Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"num_feats = 256\ni_classifier = FCLayer(num_feats, 1)\nb_classifier = BClassifier(input_size=num_feats, output_class=1)\nmodel = FullNet(i_classifier, b_classifier, feature_extractor)\n\n# load pretrained model\nlocal_checkpoint = torch.load('/kaggle/working/frozen_enc_best.pth')\nmodel.load_state_dict(local_checkpoint['model_state_dict'])\n# model = nn.DataParallel(model, device_ids=[0, 1])\nmodel = model.to(device)\n\n# print(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:03.886493Z","iopub.execute_input":"2024-03-28T02:01:03.887509Z","iopub.status.idle":"2024-03-28T02:01:03.951658Z","shell.execute_reply.started":"2024-03-28T02:01:03.887474Z","shell.execute_reply":"2024-03-28T02:01:03.950695Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# # freeze the feature extractor\nextractor_params = [param for name, param in model.named_parameters() if 'feature_extractor' in name]\nnormal_params = [param for name, param in model.named_parameters() if not 'feature_extractor' in name]\n# for name, param in model.named_parameters():\n#     if 'feature_extractor' in name:\n#         param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:05.078180Z","iopub.execute_input":"2024-03-28T02:01:05.078490Z","iopub.status.idle":"2024-03-28T02:01:05.083998Z","shell.execute_reply.started":"2024-03-28T02:01:05.078465Z","shell.execute_reply":"2024-03-28T02:01:05.083073Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam([{'params': extractor_params, 'lr': 1e-5}, {'params': normal_params, 'lr': 1e-2}], betas=(0.5, 0.9), weight_decay=1e-4)\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, 0)\n# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40,80,100], gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:14.205681Z","iopub.execute_input":"2024-03-28T02:01:14.206041Z","iopub.status.idle":"2024-03-28T02:01:14.212293Z","shell.execute_reply.started":"2024-03-28T02:01:14.206013Z","shell.execute_reply":"2024-03-28T02:01:14.211344Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# def optimal_thresh(fpr, tpr, thresholds, p=0):\n#     loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n#     idx = np.argmin(loss, axis=0)\n#     return fpr[idx], tpr[idx], thresholds[idx]\n\n# def five_scores(bag_labels, bag_predictions):\n# #     print(bag_labels, bag_predictions)\n#     fpr, tpr, threshold = roc_curve(bag_labels, bag_predictions, pos_label=1)\n#     fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n#     auc_value = roc_auc_score(bag_labels, bag_predictions)\n#     this_class_label = np.array(bag_predictions)\n#     this_class_label[this_class_label>=threshold_optimal] = 1\n#     this_class_label[this_class_label<threshold_optimal] = 0\n#     bag_predictions = this_class_label\n#     precision, recall, fscore, _ = precision_recall_fscore_support(bag_labels, bag_predictions, average='binary')\n#     accuracy = 1- np.count_nonzero(np.array(bag_labels).astype(int)- bag_predictions.astype(int)) / len(bag_labels)\n#     return accuracy, auc_value, precision, recall, fscore","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:16.488403Z","iopub.execute_input":"2024-03-28T02:01:16.489251Z","iopub.status.idle":"2024-03-28T02:01:16.493579Z","shell.execute_reply.started":"2024-03-28T02:01:16.489222Z","shell.execute_reply":"2024-03-28T02:01:16.492709Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"def test(model, dataloader):\n    bag_labels = []\n    bag_predictions = []\n    model.eval()\n    with torch.no_grad():\n        for data in dataloader:\n            images, label = data['images'], data['label']\n\n            images = torch.cat(images, dim=0).to(device)\n    #             label = label.to(device)\n\n            classes, bag_prediction, _, _ = model(images)\n#             bag_prediction = model(images)\n\n            bag_labels.append(label.numpy())\n            bag_predictions.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())\n\n#     five_scores_bag_predictions = bag_predictions\n    bag_predictions = [0 if prediction < 0.5 else 1 for prediction in bag_predictions]\n#     print(bag_labels, bag_predictions)\n\n    balanced_acc = balanced_accuracy_score(bag_labels, bag_predictions)\n    normal_acc = accuracy_score(bag_labels, bag_predictions)\n        \n    # --- Printing evaluation numbers every 5 epochs ---\n#         correct = 0\n#         for i in range(len(bag_predictions)):\n#             if bag_predictions[i] == bag_labels[i]:\n#                 correct += 1\n    print(\"Balanced Acc: {:.4f} Normal Acc: {:.4f}\".format(balanced_acc, normal_acc))\n    return balanced_acc\n    \n#     acc, auc_value, precision, recall, fscore = five_scores(bag_labels, five_scores_bag_predictions)\n#     print(acc, auc_value, precision, recall, fscore)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:16.870849Z","iopub.execute_input":"2024-03-28T02:01:16.871109Z","iopub.status.idle":"2024-03-28T02:01:16.879100Z","shell.execute_reply.started":"2024-03-28T02:01:16.871087Z","shell.execute_reply":"2024-03-28T02:01:16.878231Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/classification-dataset/test_set.csv')\ntest_path = '/kaggle/input/classification-dataset/dlmi-lymphocytosis-classification/testset'\ntest_dataset = BagDataset(test_path, test_df, val_transforms)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:17.402822Z","iopub.execute_input":"2024-03-28T02:01:17.403092Z","iopub.status.idle":"2024-03-28T02:01:17.411784Z","shell.execute_reply.started":"2024-03-28T02:01:17.403069Z","shell.execute_reply":"2024-03-28T02:01:17.410982Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# preds = []\n# model.eval()\n# with torch.no_grad():\n#     for data in test_loader:\n#         images, label = data['images'], data['label']\n\n#         images = torch.cat(images, dim=0).to(device)\n\n#         classes, bag_prediction, _, _ = model(images)\n\n#         preds.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:19.115940Z","iopub.execute_input":"2024-03-28T02:01:19.116280Z","iopub.status.idle":"2024-03-28T02:01:19.120571Z","shell.execute_reply.started":"2024-03-28T02:01:19.116254Z","shell.execute_reply":"2024-03-28T02:01:19.119705Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# preds = [0 if pred < 0.5 else 1 for pred in preds]","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:19.521329Z","iopub.execute_input":"2024-03-28T02:01:19.521585Z","iopub.status.idle":"2024-03-28T02:01:19.525315Z","shell.execute_reply.started":"2024-03-28T02:01:19.521565Z","shell.execute_reply":"2024-03-28T02:01:19.524355Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# # preds = rf.predict(X_test)\n\n# data = {'ID': test_df['ID'], 'Predicted': preds}\n# output = pd.DataFrame(data=data)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:20.423788Z","iopub.execute_input":"2024-03-28T02:01:20.424056Z","iopub.status.idle":"2024-03-28T02:01:20.427876Z","shell.execute_reply.started":"2024-03-28T02:01:20.424034Z","shell.execute_reply":"2024-03-28T02:01:20.426877Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# output.to_csv('/kaggle/working/sample_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:21.070187Z","iopub.execute_input":"2024-03-28T02:01:21.070933Z","iopub.status.idle":"2024-03-28T02:01:21.074378Z","shell.execute_reply.started":"2024-03-28T02:01:21.070906Z","shell.execute_reply":"2024-03-28T02:01:21.073501Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# test(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:21.913567Z","iopub.execute_input":"2024-03-28T02:01:21.914363Z","iopub.status.idle":"2024-03-28T02:01:21.918617Z","shell.execute_reply.started":"2024-03-28T02:01:21.914333Z","shell.execute_reply":"2024-03-28T02:01:21.917641Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_losses)\nplt.show()\nplt.savefig('/kaggle/working/loss_curve.png')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:22.974898Z","iopub.execute_input":"2024-03-28T02:01:22.975189Z","iopub.status.idle":"2024-03-28T02:01:23.216602Z","shell.execute_reply.started":"2024-03-28T02:01:22.975166Z","shell.execute_reply":"2024-03-28T02:01:23.215770Z"},"trusted":true},"execution_count":108,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"train_losses = []\nval_losses = []\naccuracies = []\nbal_accs = []\nroc_accs = []\n\nbest_acc = 0.0\nfor epoch in range(150):\n    epoch_loss = 0.0\n    model.train()\n    print(\"-- training: epoch {}\".format(epoch+1))\n#     ground_labels = []\n#     bag_predictions = []\n#     max_predictions = []\n    for data in train_loader:\n        images, label = data['images'], data['label']\n#         ground_labels.append(label)\n        images = torch.cat(images, dim=0).to(device)\n        label = label.to(device)\n\n\n        classes, bag_prediction, _, _ = model(images) # n X L\n        max_prediction, index = torch.max(classes, 0)\n#         print(bag_prediction, max_prediction)\n#         bag_predictions.append(bag_prediction)\n#         max_predictions.append(max_prediction)\n#         torch.cuda.empty_cache()\n\n#         bag_predictions = torch.cat(bag_predictions, dim=0)\n#         max_predictions = torch.cat(max_predictions, dim=0)\n        loss_bag = criterion(bag_prediction.view(1, -1), label.view(1, -1).float())\n        loss_max = criterion(max_prediction.view(1, -1), label.view(1, -1).float())\n        \n#         prediction = model(images)\n#         loss = criterion(prediction.view(1, -1), label.view(1, -1).float())\n        weight = 1.0\n        if label.item() == 1:\n#             print(\"label: {}\".format(label.item()))\n            weight_factor = 0.30\n        else:\n#             print(\"label: {}\".format(label.item()))\n            weight_factor = 0.70\n        \n        loss_total = (0.5*loss_bag + 0.5*loss_max) * weight_factor\n#         loss_total = loss * weight_factor\n#         print(type(loss_total), loss_total.shape)\n        loss_total = loss_total.mean()\n\n        optimizer.zero_grad()\n        loss_total.backward()\n        optimizer.step()  \n\n        epoch_loss += loss_total.item()\n\n    print(\"Epoch loss: {:.4f}\".format(epoch_loss))\n    train_losses.append(epoch_loss)\n#     test_loss = 0.0\n    \n\n    # ------------ Testing ------------\n    test(model, train_loader)\n    val_acc = test(model, val_loader)\n#     bag_labels = []\n#     bag_predictions = []\n#     model.eval()\n#     with torch.no_grad():\n#         for data in val_loader:\n#             images, label = data['images'], data['label']\n\n#             images = torch.cat(images, dim=0).to(device)\n#     #             label = label.to(device)\n\n#             classes, bag_prediction, _, _ = model(images)\n\n#             bag_labels.append(label.numpy())\n#             bag_predictions.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())\n\n#     bag_predictions = [0 if prediction < 0.5 else 1 for prediction in bag_predictions]\n# #     print(bag_labels, bag_predictions)\n\n#     balanced_acc = balanced_accuracy_score(bag_labels, bag_predictions)\n#     scikit_acc = accuracy_score(bag_labels, bag_predictions)\n#     roc_acc = roc_auc_score(bag_labels, bag_predictions)\n\n#     accuracies.append(scikit_acc)\n#     bal_accs.append(balanced_acc)\n#     roc_accs.append(roc_acc) \n        \n#     # --- Printing evaluation numbers every 5 epochs ---\n#     if (epoch+1) % 5 == 0:\n#         print(\"-- testing: epoch {}\".format(epoch+1))\n# #         correct = 0\n# #         for i in range(len(bag_predictions)):\n# #             if bag_predictions[i] == bag_labels[i]:\n# #                 correct += 1\n#         print(\"Balanced Acc: {:.4f} scikit-acc: {:.4f} roc_score: {:.4f}\".format(balanced_acc, scikit_acc, roc_acc))\n    \n    # --- Save the model every 50 epochs ---\n    if (epoch+1) % 50 == 0:\n        PATH = '/kaggle/working/frozen_enc_2' + str(epoch+1) + '.pth'\n        torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, PATH)\n    \n    # --- Save the best model ---\n    if val_acc > best_acc:\n        best_acc = val_acc\n        PATH = '/kaggle/working/frozen_enc_2best.pth'\n        torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, PATH)\n\n\n#     scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:01:24.179506Z","iopub.execute_input":"2024-03-28T02:01:24.179819Z","iopub.status.idle":"2024-03-28T02:12:21.308287Z","shell.execute_reply.started":"2024-03-28T02:01:24.179793Z","shell.execute_reply":"2024-03-28T02:12:21.306720Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"-- training: epoch 1\nEpoch loss: 61.1180\nBalanced Acc: 0.6403 Normal Acc: 0.6846\nBalanced Acc: 0.5478 Normal Acc: 0.6061\n-- training: epoch 2\nEpoch loss: 64.5068\nBalanced Acc: 0.6097 Normal Acc: 0.4692\nBalanced Acc: 0.5652 Normal Acc: 0.3939\n-- training: epoch 3\nEpoch loss: 61.4734\nBalanced Acc: 0.5000 Normal Acc: 0.6923\nBalanced Acc: 0.5000 Normal Acc: 0.6970\n-- training: epoch 4\nEpoch loss: 73.8939\nBalanced Acc: 0.5000 Normal Acc: 0.6923\nBalanced Acc: 0.5000 Normal Acc: 0.6970\n-- training: epoch 5\nEpoch loss: 59.4562\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 6\nEpoch loss: 60.5730\nBalanced Acc: 0.6333 Normal Acc: 0.6846\nBalanced Acc: 0.5413 Normal Acc: 0.6364\n-- training: epoch 7\nEpoch loss: 68.0532\nBalanced Acc: 0.6500 Normal Acc: 0.6692\nBalanced Acc: 0.5109 Normal Acc: 0.5152\n-- training: epoch 8\nEpoch loss: 68.8368\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 9\nEpoch loss: 63.4143\nBalanced Acc: 0.5000 Normal Acc: 0.6923\nBalanced Acc: 0.5000 Normal Acc: 0.6970\n-- training: epoch 10\nEpoch loss: 72.1076\nBalanced Acc: 0.5056 Normal Acc: 0.3154\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 11\nEpoch loss: 63.3251\nBalanced Acc: 0.5000 Normal Acc: 0.6923\nBalanced Acc: 0.5000 Normal Acc: 0.6970\n-- training: epoch 12\nEpoch loss: 64.9026\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 13\nEpoch loss: 56.5846\nBalanced Acc: 0.6333 Normal Acc: 0.7231\nBalanced Acc: 0.5065 Normal Acc: 0.6667\n-- training: epoch 14\nEpoch loss: 68.1460\nBalanced Acc: 0.6125 Normal Acc: 0.7231\nBalanced Acc: 0.5065 Normal Acc: 0.6667\n-- training: epoch 15\nEpoch loss: 70.2263\nBalanced Acc: 0.5264 Normal Acc: 0.3538\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 16\nEpoch loss: 57.9205\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 17\nEpoch loss: 71.2944\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 18\nEpoch loss: 70.5732\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 19\nEpoch loss: 72.4160\nBalanced Acc: 0.6222 Normal Acc: 0.7462\nBalanced Acc: 0.5065 Normal Acc: 0.6667\n-- training: epoch 20\nEpoch loss: 63.6325\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 21\nEpoch loss: 65.5732\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 22\nEpoch loss: 60.9851\nBalanced Acc: 0.5000 Normal Acc: 0.3077\nBalanced Acc: 0.5000 Normal Acc: 0.3030\n-- training: epoch 23\nEpoch loss: 75.7295\nBalanced Acc: 0.5806 Normal Acc: 0.4385\nBalanced Acc: 0.5435 Normal Acc: 0.3636\n-- training: epoch 24\nEpoch loss: 63.4200\nBalanced Acc: 0.5000 Normal Acc: 0.6923\nBalanced Acc: 0.5000 Normal Acc: 0.6970\n-- training: epoch 25\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[109], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- training: epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     ground_labels = []\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     bag_predictions = []\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     max_predictions = []\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m         images, label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         ground_labels.append(label)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:355\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 355\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:509\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     answer_challenge(c, authkey)\n\u001b[0;32m--> 509\u001b[0m     \u001b[43mdeliver_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:738\u001b[0m, in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthkey must be bytes, not \u001b[39m\u001b[38;5;132;01m{0!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(authkey)))\n\u001b[1;32m    737\u001b[0m message \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39murandom(MESSAGE_LENGTH)\n\u001b[0;32m--> 738\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHALLENGE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m digest \u001b[38;5;241m=\u001b[39m hmac\u001b[38;5;241m.\u001b[39mnew(authkey, message, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmd5\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[1;32m    740\u001b[0m response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mrecv_bytes(\u001b[38;5;241m256\u001b[39m)        \u001b[38;5;66;03m# reject large message\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:200\u001b[0m, in \u001b[0;36m_ConnectionBase.send_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m offset \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m>\u001b[39m n:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer length < offset + size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m:\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"plt.plot(train_losses)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:59.536881Z","iopub.status.idle":"2024-03-27T16:58:59.537304Z","shell.execute_reply.started":"2024-03-27T16:58:59.537081Z","shell.execute_reply":"2024-03-27T16:58:59.537097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss = 0.0\n\nbag_labels = []\nbag_predictions = []\nmodel.eval()\nwith torch.no_grad():\n    for data in val_loader:\n        images, label = data['images'], data['label']\n\n        images = torch.cat(images, dim=0).to(device)\n#             label = label.to(device)\n\n        classes, bag_prediction, _, _ = model(images)\n\n        bag_labels.append(label.numpy())\n        bag_predictions.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())\n\nbag_predictions = [0 if prediction < 0.5 else 1 for prediction in bag_predictions]\nprint(bag_labels, bag_predictions)\n\ncorrect = 0\nfor i in range(len(bag_predictions)):\n    if bag_predictions[i] == bag_labels[i]:\n        correct += 1\nprint(\"Accuracy: {}\".format(correct / len(bag_predictions)))","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:59.539287Z","iopub.status.idle":"2024-03-27T16:58:59.539673Z","shell.execute_reply.started":"2024-03-27T16:58:59.539486Z","shell.execute_reply":"2024-03-27T16:58:59.539503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:59.541017Z","iopub.status.idle":"2024-03-27T16:58:59.541414Z","shell.execute_reply.started":"2024-03-27T16:58:59.541222Z","shell.execute_reply":"2024-03-27T16:58:59.541238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def five_scores(bag_labels, bag_predictions):\n    print(bag_labels, bag_predictions)\n    fpr, tpr, threshold = roc_curve(bag_labels, bag_predictions, pos_label=1)\n    fpr_optimal, tpr_optimal, threshold_optimal = optimal_thresh(fpr, tpr, threshold)\n    auc_value = roc_auc_score(bag_labels, bag_predictions)\n    this_class_label = np.array(bag_predictions)\n    this_class_label[this_class_label>=threshold_optimal] = 1\n    this_class_label[this_class_label<threshold_optimal] = 0\n    bag_predictions = this_class_label\n    precision, recall, fscore, _ = precision_recall_fscore_support(bag_labels, bag_predictions, average='binary')\n    accuracy = 1- np.count_nonzero(np.array(bag_labels).astype(int)- bag_predictions.astype(int)) / len(bag_labels)\n    return accuracy, auc_value, precision, recall, fscore","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:59.543917Z","iopub.status.idle":"2024-03-27T16:58:59.544337Z","shell.execute_reply.started":"2024-03-27T16:58:59.544116Z","shell.execute_reply":"2024-03-27T16:58:59.544133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bag_labels = []\nbag_predictions = []\nwith torch.no_grad():\n#     correct = 0\n#     total = 0\n\n    for data in val_loader:\n        \n        model.eval()\n        images, label = data['images'], data['label']\n        \n        images = torch.cat(images, dim=0).to(device)\n#         label = label.to(device)\n        \n        classes, bag_prediction, _, _ = model(images)\n        \n        bag_labels.append(label.numpy())\n        bag_predictions.append(torch.sigmoid(bag_prediction).cpu().squeeze().numpy())\n        \n#         print(label, bag_prediction)\n#         if bag_prediction.item() == label.item():\n#             correct += 1\n        \n#         total += 1\n\n# print(correct/total)\nacc, auc, p, r, f = five_scores(bag_labels, bag_predictions)\n\nprint(acc, auc, p, r, f)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:58:59.546167Z","iopub.status.idle":"2024-03-27T16:58:59.546672Z","shell.execute_reply.started":"2024-03-27T16:58:59.546403Z","shell.execute_reply":"2024-03-27T16:58:59.546424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}